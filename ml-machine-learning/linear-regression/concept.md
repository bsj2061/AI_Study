---
description: 선형회귀(Linear Regression)에 대한 기본적인 개념을 알아보는 페이지입니다.
---

# Concept

## 1) 선형회귀(Linear Regression)

&#x20;선형회귀란 학습을 통해 독립변수 x와 그에 대한 종속변수 y의 관계를 가장 잘 나타낼 수 있는 선형 모델로 나타내는 것을 말한다. 선형회귀를 통해 나온 선형 모델로 데이터들의 경향성을 파악하여 다른 입력값에 대한 출력값을 예측하는 데에 사용된다.



### 단순 선형 회귀(Simple Linear Regression)

&#x20; 단순 선형 회귀는 종속변수 y에 영향을 끼치는 독립변수 x의 수가 한 개인 경우로 다음과 같은 수식으로 선형모델을 정의한다.

&#x20;                                                                        $$y  = wx + b$$

&#x20;  여기에서 $$w$$는 가중치(weight)를 의미하며 기울기를 나타내고 $$b$$는 편향(bias)을 의미하며 y절편을 나타낸다.



### 다변량 선형 회귀(Multivariate Linear Regression)

&#x20;종속변수 y의 값을 결정하는 속성이 단순 선형 회귀의 상황에서처럼 하나가 아닐 때 다변량 선형 회귀를 사용하며 다음과 같은 수식으로 정의한다.

&#x20;                                           $$y = w_1x_1+w_2x_2+w_3x_3+... +w_nx_n+b$$               &#x20;

여기에서 $$w_i$$는 종속변수 y의 i번째 속성을 의미하고 $$b$$는 편향을 의미한다.



## 2)최소제곱법(LSM)

### 평균제곱오차(MSE)

최소제곱법에 대해 알기 전에 평균제곱오차에 대해서 알 필요가 있다. 평균제곱오차는 예측값에 실제 데이터를 빼서 구한 오차를 제곱한 것의 평균으로 식으로 나타내면 다음과 같다.

&#x20;                                                               $$\frac{1}{N}\sum_{i=1}^N (f(x_i)-y_i)^2$$

여기에서 $$f(x_i)$$는 독립변수 $$x_i$$에 대한 예측값을 의미하고 $$y_i$$는 $$x_i$$에 대한 실제 데이터값을 의미한다. 이것은 기하학적으로 점$$(x_i,f(x_i))$$와 점$$(x_i,y_i)$$의 유클라디안 거리의 평균을 의미한다.&#x20;

![](<../../.gitbook/assets/image (4).png>)

따라서 평균제곱오차를 최소화 하는 방법으로 모델의 해를 구해야하는데 이것을 최소제곱법이라고 한다.

### 최소제곱법(LSM)-단순 선형 회귀&#x20;

최소제곱법이란 앞서 말했듯이 평균제곱오차를 최소화하는 방법으로 모델의 해를 구하는 방법을 말한다. 선형회귀에서 최소제곱법은 실제 데이터와의 유클리드거리 합이 가장 작은 하나의 직선을 찾는 것을 목표로 한다.  $$w$$와 $$b$$의 해를 찾는 것은 다음 식 $$E_{(w,b)}$$을 최소화하는 것을 말한다.

&#x20;                                              $$E_{(w,b)}=\sum_{i=1}^N(y_i-wx_i-b)^2$$  &#x20;

여기에서 $$y_i$$는 $$x_i$$에 해당하는 실제 데이터값을 의미하고 w는 가중치(weight)를 b는 편향(bias)을 의미한다. $$E_{(w,b)}$$를 최소화하기 위하여 이 식 $$E_{(w,b)}$$을 $$w$$와 $$b$$에 대해 편미분하면 다음과 같다.

&#x20;                                    $$\frac{\partial E_{(w,b)}}{\partial w} = 2(w\sum_{i=1}^N x_i^2-\sum_{i=1}^N(y_i-b)x_i)$$&#x20;

&#x20;                                    $$\frac{\partial E_{(w,b)}}{\partial b}=2(Nb-\sum_{i=1}^N(y_i-wx_i))$$

그리고 다음 식을 통해 $$w$$와 $$b$$의 최적해(optimum solution)의 닫힌 해(closed-form solution)를 구할 수 있다.

&#x20;                                         $$w= \frac{\sum_{i=1}^N y_i(x_i-\bar{x})}{\sum_{i=1}^N x_i^2-\frac{1}{N}(\sum_{i=1}^N x_i)^2}$$ &#x20;

&#x20;                                          $$b=\frac{1}{N}\sum_{i=1}^N(y_i-wx_i)$$

여기에서 $$\bar{x}$$는 x의 평균값을 의미한다.

### 최소제곱법(LSM)-다변량 선형회귀

&#x20;일반적인 상황에서 종속변수에 영향을 미치는 속성은 단순 선형 회귀에서처럼 하나만 존재하지 않는다. 보통 여러 개의 속성이 종속변수에 영향을 미치는데 $$d$$개의 속성을 가진 샘플 데이터셋 D가 존재할 때 이것을 행렬을 통해 표현하면 다음과 같다.

&#x20;                                            $$f(x_i) = w^T\bold{x}_i+b$$&#x20;

&#x20;앞서 살펴본 단순 선형회귀와 비슷하게 다변량 선형회귀에서도 최소제곱법을 이용하여 $$\bold{w}$$와 $$b$$를 추정할 수 있다. 간단히 하기위해 $$\hat{\bold{w}}=(\bold{w}; b)$$를 만들고 데이터셋 D는 $$m$$x$$(d+1)$$ 크기의 행렬 X로 놓고 각 행은 하나의 샘플을 의미한다고 하자. 각 행의 $$d$$개의 원소는 샘플이 가지고 있는 $$d$$개의 속성값을 나타내고 마지막 열에는 원소 1을 나열하면 행렬 $$X$$는 다음과 같다.

&#x20;                         $$X=\begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1d} & 1 \\ x_{21} & x_{22} & \cdots & x_{2d} & 1 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ x_{m1} & x_{m2} & \cdots & x_{md} & 1 \end{pmatrix} = \begin{pmatrix} \bold{x}_1^T & 1 \\ \bold{x}_2^T & 1 \\ \vdots & \vdots \\ \bold{x}_m^T & 1 \end{pmatrix}$$

종속변수 $$y$$를 벡터형식인 $$y=(y_1;y_2;... ;y_m)$$으로 바꾸어 적절하게 변환해주면 다음과 같다.

&#x20;                                $$\hat{\bold{w}}^* = \underset{\hat{\bold{w}}}{argmin}(\bold{y}-X\hat{\bold{w}})^T(\bold{y}-X\hat{\bold{w}})$$

$$E_\hat{w} = (y-X\hat{w})^T(y-X\hat{w})$$에서 최적의 $$\hat{w}$$를 구하기 위해 다음 식을 이용한다.

&#x20;                                                 $$\frac{\partial E_{\hat{\bold{w}}}}{\partial \hat{\bold{w}}} = 2X^T(X\hat{\bold{w}}-\bold{y})$$

위 식이 0이 되도록하면 $$\hat{\bold{w}}$$ 최적해의 닫힌 해를 구할 수 있다. $$X^TX$$가 풀랭크 행렬(full-rank matrix)이거나 정치 행렬(positive definite matrix)일 때 이전 식에서 다음 식을 도출할 수 있다.

&#x20;                                                   $$\hat{\bold{w}}^* = (X^TX)^{-1}X^T\bold{y}$$

$$\hat{\bold{x}_i} = (x_i; 1)$$일 경우  최종 학습 된 다중 선형 회귀 모델은 다음이 된다.

&#x20;                                                 $$f(\hat{\bold{x}_i}) = \hat{\bold{x}_i}^T(X^TX)^{-1}X^T\bold{y}$$

하지만 현실에서 $$X^TX$$가 풀랭크 행렬인 경우는 많지 않다. 이때 다수의 $$\hat{\bold{w}}$$를 구하고 평균제곱오차를 최소화할 수 있지만 정규화(regularization)같은 방법을 사용해야 한다.

## 3)경사하강법(Gradient Descent)

&#x20;머신러닝이나 딥러닝에서 많은 경우에 비용함수(cost function)를 최소화하는 $$w$$와 $$b$$를 찾기 위한 작업을수행한다. 이때 사용되는 알고리즘을 최적화 알고리즘이라고 부른다. 경사하강법은 가장 기본적인 최적화 알고리즘 중 하나이다.&#x20;

&#x20;경사하강법의 기본적인 원리는 함수값이 낮아지는 방향으로 독립변수 값을 변형시켜 가면서 최종적으로는 함수의 최솟값을 갖도록 하는 독립변수를 찾는 것이다. 이때 '함수의 최댓값, 최솟값을 찾기 위해서 미분계수가 0인 지점을 찾으면 되지 않는가?'하는 의문을 가질 수 있지만 경사하강법을 사용하는 주된 이유는 다음과 같다.

* 우리가 주로 실제 분석에서 맞딱드리게 되는 함수들은 닫힌 형태(closed form)가 아니거나 함수의 형태가 복잡해 (비선형함수) 미분계수와 그 근을 계산하기 어려운 경우가 많다.
* 실제 미분계수를 계산하는 과정을 컴퓨터로 구현하는 것에 비해 경사하강법은 컴퓨터로 비교적 쉽게 구현할 수 있다.
* 데이터 양이 매우 큰 경우 경사하강법과 같은 반복적인 방법을 통해 해를 구하면 계산량 측면에서 더 효율적으로 해를 구할 수 있다.

경사하강법은 기울기가 음수이면 함수가 감소하고 양수이면 함수가 증가한다는 점과 기울기가 가파르면 최솟값이나 최댓값에 해당하는 점과 멀리 떨어져있다는 점을 이용한다.

&#x20;

![](<../../.gitbook/assets/image (2).png>)

앞서 말한 기울기(gradient)의 방향과 크기를 종합하여 경사하강법에 적용하면 다음과 같은 수식을 얻을 수 있다.

&#x20;                                                              $$x_{i+1} = x_i-\alpha\frac{df}{dx}(x_i)$$

여기에서 $$\alpha$$는 이동 거리의 조정값으로 보통 step size라고 부른다. 앞선 식을 다변수 함수에 대해 확장하면 다음 식과 같다.

&#x20;                                                               $$x_{i+1} = x_i - \alpha\nabla f(x_i)$$

여기에서 $$\nabla$$는 델 연산자로 불리며 $$n$$차원 공간에서 다음과 같이 정의된다.

&#x20;                                                                $$\nabla = \underset{i=1}{\overset{n}\sum}e _i\frac{\partial}{\partial x_i}$$

여기에서 $$e_i$$는 $$i$$번째 좌표만 1이고 나머지는 0으로 채워진 $$n$$차원의 표준기저를 말한다.

### 적절한 Step Size 설정의 필요성

Step Size는 학습률이라고도 불린다. Step Size가 큰 경우 이동거리가 커져서 빠르게 수렴할 수 있다는 장점이 있지만, 최솟값으로 수렴하지 못하고 함수값이 발산할 여지가 있다. 한편 Step Size가 작으면 발산하지는 않겠지만 최솟값을 찾는데 너무 많은 시간이 소요될 여지가 있다.

&#x20;

![](<../../.gitbook/assets/image (12).png>)

따라서 경사하강법에서 적절한 크기의 Step Size를 설정하는 것이 중요하다.
