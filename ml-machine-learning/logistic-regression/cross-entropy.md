---
description: 크로스엔트로피(Cross Entropy)에 대해 알아보는 페이지입니다.
---

# Cross entropy

## 1) 엔트로피(Entropy)

엔트로피란 샘플 집합의 순도를 측정하는 데 가장 자주 사용되는 일종의 지표이다. 정보이론에서 엔트로피는 불확실성을 의미하며 엔트로피가 높을수록 정보가 많고 확률이 낮다는 것을 의미한다. $$P(x)$$를 $$x$$가 발현될 확률이라 하고 $$I(x)$$를 정보량이라고 하면 정보량 $$I(x)$$는 Eq.1과 같다.

$$I(x) =-  log_2P (x)$$                                Eq.1

여기서 엔트로피는 각 사건의 발생확률과 각 사건의 정보량의 곱을 모두 합한 것이므로 엔트로피는 Eq.2와 같다.&#x20;

$$H(P  )=-\overset{n}{\underset{i =1}{\sum }} P_ilog_2P_i$$                         Eq.2

## 2) 크로스엔트로피(Cross Entropy)

원래 크로스 엔트로피란 실체 분포 $$Q$$를 모르고, 모델링을 통해 $$Q$$분포를 예측하고자 하는 것이다.  예측 모델링을 통해 구한 분포를 $$P( x)$$라 하고, 실제분포인 $$Q( x)$$를 예측하는 $$P (x)$$분포를 만들었을 떄 크로스 엔트로피는 Eq.3와 같이 정의된다.

$$H(P, Q)=-\overset{n}{\underset{i=1}{\sum}} P_ilog_2Q_i$$                   Eq.3

여기에서 $$P$$와 $$Q$$가 모두 들어가서 크로스엔트로피라고 한다.

## 3) Kullback-Leibler Divergence(KL Divergence)

KL Divergence는 서로 다른 두 확률분포의 차이를 측정하는데 쓰이는 함수이다. 이를 엔트로피와 크로스엔트로피 개념에 대입하면 두 엔트로피의 차이로 계산된다. 크로스엔트로피는 특히 분류문제에서 CostFunction으로 사용되는데 그 이유가 여기에 있다. 우선 $$P$$와 $$Q$$가 이산 분포확률이라고 한다면, KL Divergence의 정의는 Eq.2과 Eq.3의 차인 Eq.4와 같다. ($$P$$와 $$Q$$가 연속분포확률이었다면, Eq.4에서 시그마 대신에 integral이 들어간다.)

$$D_{KL}(P||Q)=H(P,Q)-H(P) =   \overset{n}{\underset{i=1}{\sum}}P_ilog_2\frac{P_i}{Q_i}$$          Eq.4&#x20;

실제 확률분포 크로스엔트로피는 항상 엔트로피보다 크므로 KL Divergence는 항상 0보다 더 큰 값이다. 예측분포와 실제분포를 가깝게 하는것이 예측모형이 하고자 하는 것이며 예측분포와 실제분포가 가까워 진다면 KL Divergence 값은 0에 가까워질 것이다. 이러한 이유로 크로스엔트로피는 머신러닝의 분류문제에서 Cost Function으로 자주 사용되고 있다.&#x20;
